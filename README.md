# Minlayers

The goal is to implement a [torch.nn](https://pytorch.org/docs/stable/nn.html) from PyTorch in the simplest way 
possible and with most of its features.
Each layer should work with pytorch api.

Inspired by [nn-zero-to-hero](https://github.com/karpathy/nn-zero-to-hero)


# How to use

To plot a activation function

arg:

- help - to see all available functions

- all - to plot all functions

- type name of function

```bash

python3 plot_act.py [arg]

```

# Papers

Activations: [HardShrink](https://arxiv.org/pdf/1312.6120.pdf), [ELU](https://arxiv.org/pdf/1511.07289.pdf), 
[ReLU6](https://arxiv.org/pdf/1704.04861.pdf), [Swish](https://arxiv.org/pdf/1710.05941.pdf), [Mish](https://arxiv.org/pdf/1908.08681.pdf),
[GELU](https://arxiv.org/pdf/1606.08415.pdf)

Others: [Dropout](https://arxiv.org/pdf/1207.0580.pdf), [BatchNorm](http://arxiv.org/pdf/1502.03167), [LayerNorm](https://arxiv.org/pdf/1607.06450.pdf) 

